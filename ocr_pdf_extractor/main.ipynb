{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR PDF Extractor\n",
    "This notebook extracts tables from PDF files using PaddleOCR (Hindi + English support).\n",
    "\n",
    "## Workflow:\n",
    "1. Convert PDF pages to images\n",
    "2. Run OCR on each image\n",
    "3. Reconstruct tables from OCR results\n",
    "4. Export to Excel and CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\OneDrive\\Desktop\\Projects_Archive\\pythonFiles\\ocr_pdf_extractor\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[33mChecking connectivity to the model hosters, this may take a while. To bypass this check, set `DISABLE_MODEL_SOURCE_CHECK` to `True`.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pdf2image import convert_from_path\n",
    "from paddleocr import PaddleOCR\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "Adjust these settings as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration set and directories created\n"
     ]
    }
   ],
   "source": [
    "# ================== CONFIG ==================\n",
    "PDF_PATH = \"input.pdf\"\n",
    "IMG_DIR = \"output/images\"\n",
    "EXCEL_OUT = \"output/tables.xlsx\"\n",
    "CSV_OUT = \"output/tables.csv\"\n",
    "DPI = 300\n",
    "ROW_Y_THRESHOLD = 25   # controls row grouping\n",
    "# ============================================\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(IMG_DIR, exist_ok=True)\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Configuration set and directories created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize OCR Model\n",
    "This will download the Hindi + English models if not already cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Initializing OCR model (Hindi + English)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\OneDrive\\Desktop\\Projects_Archive\\pythonFiles\\ocr_pdf_extractor\\.venv\\Lib\\site-packages\\paddle\\utils\\cpp_extension\\extension_utils.py:718: UserWarning: No ccache found. Please be aware that recompiling all source files may be required. You can download and install ccache from: https://github.com/ccache/ccache/blob/master/doc/INSTALL.md\n",
      "  warnings.warn(warning_message)\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_doc_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\anubh\\.paddlex\\official_models\\PP-LCNet_x1_0_doc_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('UVDoc', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\anubh\\.paddlex\\official_models\\UVDoc`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-LCNet_x1_0_textline_ori', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\anubh\\.paddlex\\official_models\\PP-LCNet_x1_0_textline_ori`.\u001b[0m\n",
      "\u001b[32mCreating model: ('PP-OCRv5_server_det', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\anubh\\.paddlex\\official_models\\PP-OCRv5_server_det`.\u001b[0m\n",
      "\u001b[32mCreating model: ('devanagari_PP-OCRv5_mobile_rec', None)\u001b[0m\n",
      "\u001b[32mModel files already exist. Using cached files. To redownload, please delete the directory manually: `C:\\Users\\anubh\\.paddlex\\official_models\\devanagari_PP-OCRv5_mobile_rec`.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OCR model initialized\n"
     ]
    }
   ],
   "source": [
    "print(\"üîπ Initializing OCR model (Hindi + English)...\")\n",
    "ocr = PaddleOCR(\n",
    "    lang=\"hi\",\n",
    "    use_textline_orientation=True\n",
    ")\n",
    "print(\"‚úÖ OCR model initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert PDF to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Converting PDF to images...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÑ Converting PDF to images...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m pages = \u001b[43mconvert_from_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mPDF_PATH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDPI\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpoppler_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mpoppler-25.12.0\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mLibrary\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mbin\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      6\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m image_paths = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pages):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anubh\\OneDrive\\Desktop\\Projects_Archive\\pythonFiles\\ocr_pdf_extractor\\.venv\\Lib\\site-packages\\pdf2image\\pdf2image.py:251\u001b[39m, in \u001b[36mconvert_from_path\u001b[39m\u001b[34m(pdf_path, dpi, output_folder, first_page, last_page, fmt, jpegopt, thread_count, userpw, ownerpw, use_cropbox, strict, transparent, single_file, output_file, poppler_path, grayscale, size, paths_only, use_pdftocairo, timeout, hide_annotations)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m uid, proc \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[32m    250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m         data, err = \u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired:\n\u001b[32m    253\u001b[39m         proc.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\subprocess.py:1222\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1219\u001b[39m     endtime = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1221\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     stdout, stderr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1224\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1225\u001b[39m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[32m   1226\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\subprocess.py:1644\u001b[39m, in \u001b[36mPopen._communicate\u001b[39m\u001b[34m(self, input, endtime, orig_timeout)\u001b[39m\n\u001b[32m   1640\u001b[39m \u001b[38;5;66;03m# Wait for the reader threads, or time out.  If we time out, the\u001b[39;00m\n\u001b[32m   1641\u001b[39m \u001b[38;5;66;03m# threads remain reading and the fds left open in case the user\u001b[39;00m\n\u001b[32m   1642\u001b[39m \u001b[38;5;66;03m# calls communicate again.\u001b[39;00m\n\u001b[32m   1643\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1644\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstdout_thread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_remaining_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1645\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout_thread.is_alive():\n\u001b[32m   1646\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TimeoutExpired(\u001b[38;5;28mself\u001b[39m.args, orig_timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\uv\\python\\cpython-3.13.5-windows-x86_64-none\\Lib\\threading.py:1094\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     timeout = \u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"üìÑ Converting PDF to images...\")\n",
    "pages = convert_from_path(\n",
    "    PDF_PATH,\n",
    "    dpi=DPI,\n",
    "    poppler_path=r\"C:\\poppler-25.12.0\\Library\\bin\"\n",
    ")\n",
    "\n",
    "image_paths = []\n",
    "for i, page in enumerate(pages):\n",
    "    path = f\"{IMG_DIR}/page_{i+1}.png\"\n",
    "    page.save(path, \"PNG\")\n",
    "    image_paths.append(path)\n",
    "\n",
    "print(f\"‚úÖ {len(image_paths)} pages converted\")\n",
    "print(f\"üìÅ Images saved to: {IMG_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preview First Page (Optional)\n",
    "Uncomment to display the first converted page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image, display\n",
    "# if image_paths:\n",
    "#     display(Image(filename=image_paths[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OCR Processing & Table Reconstruction\n",
    "Process each page (skipping the first page) and reconstruct tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m all_rows = []\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(image_paths[\u001b[32m1\u001b[39m:], desc=\u001b[33m\"\u001b[39m\u001b[33müîç OCR Processing\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      4\u001b[39m     result = ocr.predict(img_path)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result[\u001b[32m0\u001b[39m]:\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "\n",
    "for img_path in tqdm(image_paths[1:], desc=\"üîç OCR Processing\"):\n",
    "    result = ocr.predict(img_path)\n",
    "\n",
    "    if not result or not result[0]:\n",
    "        continue\n",
    "\n",
    "    rows = {}\n",
    "\n",
    "    # PaddleOCR predict returns: [[box, (text, confidence)], ...]\n",
    "    for item in result[0]:\n",
    "        if not item or len(item) < 2:\n",
    "            continue\n",
    "            \n",
    "        box = item[0]  # Bounding box coordinates\n",
    "        text_info = item[1]  # (text, confidence) tuple\n",
    "        \n",
    "        # Extract text from tuple\n",
    "        text = text_info[0] if isinstance(text_info, tuple) else text_info\n",
    "\n",
    "        x = int(box[0][0])\n",
    "        y = int(box[0][1])\n",
    "\n",
    "        row_key = y // ROW_Y_THRESHOLD\n",
    "        rows.setdefault(row_key, []).append((x, text))\n",
    "\n",
    "    for r in rows.values():\n",
    "        r.sort(key=lambda x: x[0])\n",
    "        row_text = [cell[1] for cell in r]\n",
    "\n",
    "        if len(row_text) >= 4:\n",
    "            all_rows.append(row_text)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(all_rows)} rows from {len(image_paths)-1} pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Normalize Column Count\n",
    "Ensure all rows have the same number of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_rows:\n",
    "    max_cols = max(len(r) for r in all_rows)\n",
    "    print(f\"üìä Maximum columns detected: {max_cols}\")\n",
    "\n",
    "    normalized = []\n",
    "    for row in all_rows:\n",
    "        row += [\"\"] * (max_cols - len(row))\n",
    "        normalized.append(row)\n",
    "\n",
    "    df = pd.DataFrame(normalized)\n",
    "    print(f\"‚úÖ Created DataFrame with {len(df)} rows and {len(df.columns)} columns\")\n",
    "else:\n",
    "    df = pd.DataFrame()\n",
    "    print(\"‚ö†Ô∏è No rows extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Preview Results\n",
    "Display the first few rows of the extracted table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"No data to display\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Output Files\n",
    "Export to Excel and CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    df.to_excel(EXCEL_OUT, index=False, header=False)\n",
    "    df.to_csv(CSV_OUT, index=False, header=False)\n",
    "\n",
    "    print(\"\\nüéâ DONE!\")\n",
    "    print(f\"üìä Excel saved ‚Üí {EXCEL_OUT}\")\n",
    "    print(f\"üìÑ CSV saved   ‚Üí {CSV_OUT}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Statistics & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df.empty:\n",
    "    print(\"üìà Summary Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Total pages processed: {len(image_paths)-1}\")\n",
    "    print(f\"   ‚Ä¢ Total rows extracted: {len(df)}\")\n",
    "    print(f\"   ‚Ä¢ Total columns: {len(df.columns)}\")\n",
    "    print(f\"   ‚Ä¢ Non-empty cells: {df.astype(bool).sum().sum()}\")\n",
    "    print(f\"   ‚Ä¢ Empty cells: {(df == '').sum().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
