## RDBMS

#RDBMS stands for Relational Database Management System. It is a type of database management system that stores data in a structured format using rows and columns. RDBMS uses Structured Query Language (SQL) for querying and managing the data.  
It supports SQL based on sets and relationships.

#Transactions in RDBMS are ACID compliant, meaning they ensure Atomicity, Consistency, Isolation, and Durability.

#Constraints in RDBMS are rules that enforce data integrity and consistency.

#Transaction logs are used to record all changes made to the database, allowing for recovery in case of a failure.
when something goes wrong, we can use transaction logs to rollback the changes made to the database. they improve recovery.
For example, transactions happen throughout the day, and at the end of the day, we do backup. if anything goes wromg the next dayk, we can use  previous
night backup to do a #rollback.

#Security in RDBMS is implemented through user roles and permissions, ensuring that only authorized users can access or modify the data.

#Triggers in RDBMS are special procedures that automatically execute in response to certain events on a table or view, such as insertions, updates, or deletions. They are used to enforce business rules or maintain data integrity.

## DATA WAREHOUSE

What is Data WearHouse?
A Data Warehouse is a centralized repository that stores large volumes of structured and semi-structured data from multiple sources.
business users preffer Data WearHouse over traditional databases because it is optimized for query and analysis, allowing for faster retrieval of data and better performance 
on complex queries.

what is information?
it is processed data that is meaningful and useful for decision-making. Information is derived from raw data through various processes such as filtering, sorting, and aggregating.

What  are the advantages of Data WearHouse?
1. Improved data quality and consistency: Data Warehouses integrate data from various sources, ensuring that the data is accurate, consistent, and up-to-date.
2. Enhanced data analysis and reporting: Data Warehouses support complex queries and analytics, enabling users to gain insights and make informed decisions based on the data.
3. Historical data storage: Data Warehouses can store large volumes of historical data, allowing for trend analysis and long-term decision-making.
4. Scalability: Data Warehouses can handle large volumes of data and can be scaled up. 

From query to result
take data from various sources -> integrate data  from multiple sources -> standardize data & remove inconsistencies -> store data in format suitable for easy access

## Properties of Data WearHouse

A data wearhouse is a "subject-oriented", "integrated", "time-variant", and "non-volatile" collection of data that supports decision-making processes. 
    -Bill Inmon, Father of data warehousing


## OLTP(DB) vs OLAP(DW)

| Feature                | OLTP (DB)                                  | OLAP (DW)                                 |
||--|-|
| Purpose                | Transaction processing                     | Analytical processing                     |
| Data Structure         | Highly normalized, detailed data           | Denormalized, aggregated data             |
| Operations             | Insert, update, delete, simple queries     | Complex queries, data analysis, reporting |
| Users                  | Clerks, database administrators            | Business analysts, decision makers        |
| Data Volume            | Smaller, current data                      | Large, historical data                    |
| Response Time          | Fast for transactions                      | Fast for complex queries                  |
| Example                | Banking transactions, order entry systems  | Sales trend analysis, forecasting         |


## DATA MART 

Data mart is a smaller version of a data wearhouse, less sources of data, used to satify only a certain department or business unit.
Data mart is like sub parts of Data wearhouse divides to give access to specific departments or business units.

#Difference between Data Warehouse and Data Mart:

| Feature                | Data Warehouse                              | Data Mart                                 |
|||-|
| Scope                  | Organization-wide                           | Department or business unit specific      |
| Data Sources           | Multiple, enterprise-wide                   | Few, focused on specific area             |
| Size                   | Large (terabytes to petabytes)              | Smaller (gigabytes to terabytes)          |
| Complexity             | High, integrates data from many sources     | Lower, simpler structure                  |
| Implementation Time    | Longer                                      | Shorter                                   |
| Users                  | Organization-wide decision makers           | Specific department users                 |
| Maintenance            | More complex                                | Easier                                    |
| Cost                   | Higher                                      | Lower                                     |

There are two approaches to Data Mart:
    top down approach: Data Mart is created from Data Warehouse, data is extracted from Data Warehouse and loaded into Data Mart.
    bottom up approach: Data Mart is created first, data is extracted from operational systems and loaded into Data Mart,
                        then Data Mart is integrated into Data Warehouse.

# Dependent data mart vs Independent data mart vs Hybrid data mart

> in dependent data mart, data is extracted from operational systems and loaded into Data wearhouser and then into data mart.
> in independent data mart, data is extracted from operational systems and loaded directly into data mart.
> in hybrid data mart, data is extracted from both operational systems and Data Warehouse and loaded into data mart.

## METADATA

    - Metadata is data about data. 
    - It provides information about the structure, content, and context of the data stored in a Data Warehouse or Data Mart.
    - Metadata is define where the data is coming from, how it is transformed, and how it is stored.
    - used to define what is source and what is target.

## DATA WEARHOUSE ARCHITECTURE

Database, Flat Files -> Staging Area (Staging Database) -> Data Warehouse(Meta Data, Aggregate Data, Raw Data) -> Data Marts(like Sales, Marketing, Finance) -> End Users (Business Analysts, Decision Makers)


## HANDS ON PRACTICE

# Oracle SQL Developer - Creating two tables  

# Talend Data integration - import the data from the oracle database and load it into the staging area, apply filter, create one accepted excel and the rejected ones into another excel.

### END OF LECTURE 1 UNDER SESSION 1 ###


## DATA LAKES

Started as Datalake ended up as Data Swamp.

Scaled out Data Platform. Database contains external Files.
Data Lakes optain data from Structured, un structured, and semi-structured sources.

Data from Data Lake can be placed in Data WearHouse using ETL (Extract, Transform, Load) or ELT (Extract, Load, Transform) processes.

data lake supports
    - SQL Language
    - transactions
    - ACID compliance
    - Transaction logs
    - some contains
    - almost no Security
    - no triggers
    - no backups
    - supports schema evolutions

### END OF LECTURE 2 UNDER SESSION 1 ###


## ETL (Extract, Transform, Load)

Extract - 
    - Extract data from various sources such as databases, flat files, APIs, etc.
    - Data can be structured, semi-structured, or unstructured.
    - Data is extracted in its raw form.
    - Data quality rules
    - Data Format consistency (For example converting date formats, currency formats, etc.)
    - Data Dump from source systems
    - No Complex Logics (simple logic like extracting age from date of birth is okay, but not complex logics like calculating profit from sales and expenses)
    - No running business Quaries, not given access to business department.
    - Popular Load Strategies - Full and Delta Load
            - Full Load: All data is extracted and loaded into the target system.
            - Delta Load: Only the changes (new or updated records) since the last load are extracted and loaded into the target system.
    - Load strategies - Historical and Incremental Load
            - Historical Load: All historical data is loaded into the target system.
            - Incremental Load: Only new or changed data is loaded into the target system since the last load.
    
Transform -
    - Data is cleaned, filtered, and transformed to meet the requirements of the target system.
    - Convert raw data into meaningful information.
    - Steps are 
            - mapping the data from source to target (colulmn level maping, as-is, new derived colums, rename columns)
            - Enrich the data (adding more info to make the output more meaningful, someone sends zip code, we can add city name to it)
            - Join data from multiple sources (joining data from multiple tables, or files, or databases)
            - Filter data(keeping only the required data,)
            - Remove duplicates (removing duplicate records from the data, if continious duplicate , check source or could be multiple run of a job)
            - Aggregation (summarizing the data, like calculating total sales, average sales, etc.)

Load -
    - Dimension Tables (primary key, Functional identifier, Attributes, Load Strategies, Gain)
    - Fact Tables (primary key, Foreign key, Measures)
    - EDW [Enterprise Data Warehouse] (Data Mart, Data Warehouse, Data Lake)
    - Data Mart (Sales, Marketing, Finance, etc.)


### END OF LECTURE 3 UNDER SESSION 1 ###



#Key Considerations When Choosing ETL Tools

- Data Integration Extent: Range of supported connectors
- Customizability Level: Technical flexibility vs. ease of use
- Cost Structure: Tool cost plus infrastructure and maintenance
- Automation Level: How much manual work is required
- Security & Compliance: Data protection features
- Performance & Reliability: Tool stability and speed

#Top ETL Tools Categories

# Open Source Options
- Apache Airflow: Workflow orchestration with DAGs
- Talend Open Studio: User-friendly GUI with drag-and-drop
- Pentaho Data Integration: Real-time ETL capabilities
- Hadoop: Big data processing framework
- Airbyte: Leading open-source ELT platform with 350+ connectors

# Cloud-Native Solutions
- AWS Glue: Serverless ETL with multiple interface options
- Azure Data Factory: Extensive connectors and flexible interfaces
- Google Cloud Dataflow: Serverless with Apache Beam integration

# Enterprise Tools
- IBM InfoSphere DataStage: High-speed processing with parallelization
- Oracle Data Integrator: Comprehensive database integration
- Microsoft SSIS: Built-in transformations for SQL Server environments
- Informatica PowerCenter: Industry leader with extensive connectors

# Modern SaaS Platforms
- Fivetran: Automated data replication with minimal setup
- Stitch: Simple ETL for basic transformations
- Hevo: Low-code platform with real-time integration
- Matillion: Cloud-native with visual interface

## Key Trends

- ELT vs ETL: ELT (Extract, Load, Transform) is gaining popularity over traditional ETL
- Cloud Migration: Most tools are moving toward cloud-native architectures
- Automation Focus: Modern tools emphasize automated schema detection and management
- Real-time Processing: Increasing demand for streaming data capabilities

The choice of ETL tool depends on your organization's technical expertise, data volume, budget, and specific integration requirements.

### END OF LECTURE 4 UNDER SESSION 1 ###


## INFORMATICA

# Informatica Architecture

Informatica PowerCenter follows a Service-Oriented Architecture (SOA) with these core components:

# Domain Architecture

Domain (Administrative Layer)
├── PowerCenter Repository Service
├── PowerCenter Integration Service  
├── PowerCenter Reporting Service
└── PowerCenter Web Services Hub

# Key Components:
    - Domain: Administrative layer managing all services
    - Nodes: Physical/virtual machines running Informatica services
    - Services: Processes that perform specific functions
    - Repository: Metadata storage for all objects and configurations

# Informatica PowerCenter & Repository

# PowerCenter Components:
    1. Repository Service - Manages metadata and repository database
    2. Integration Service - Executes workflows and sessions
    3. Reporting Service - Generates reports and logs
    4. Web Services Hub - Provides web service interfaces

# Repository Structure:

Repository Database
├── Metadata Tables (Source/Target definitions)
├── Transformations (Business logic)
├── Mappings (Data flow design)
├── Sessions (Runtime instructions)
├── Workflows (Process orchestration)
└── Security & User Management

# Repository Types:
    - PowerCenter Repository: Stores metadata for mappings, sessions, workflows
    - Global Repository: Shared objects across multiple domains
    - Local Repository: Domain-specific objects


# Informatica PowerCenter Designer
    - Purpose: Visual development environment for creating data mappings

# Key Features:
    - Source Analyzer: Import/define source structures
    - Warehouse Designer: Create target table structures  
    - Transformation Developer: Build reusable transformations
    - Mapplet Designer: Create reusable mapping components
    - Mapping Designer: Design complete data flows

# Common Transformations:

Source → Filter → Expression → Lookup → Aggregator → Target

# Designer Workflow:
    1. Import source definitions
    2. Create target definitions
    3. Build transformations
    4. Connect data flow
    5. Validate mapping
    6. Save to repository

# PowerCenter Workflow Manager
    - Purpose: Create and manage workflows that orchestrate ETL processes

# Task Types:
    - Session Task: Executes mappings
    - Command Task: Runs shell/batch commands
    - Email Task: Sends notifications
    - Timer Task: Adds delays
    - Decision Task: Conditional branching
    - Assignment Task: Sets workflow variables

# Workflow Objects:

    Workflow
    ├── Start Task (Entry point)
    ├── Session Tasks (Execute mappings)
    ├── Link Conditions (Control flow)
    ├── Events (Success/Failure handling)
    └── Worklets (Reusable workflow components)


# Session Configuration:
    - Mapping: Associates with created mapping
    - Source/Target Connections: Database connections
    - Session Properties: Performance and runtime settings
    - Config Object: Environment-specific configurations

## 5. PowerCenter Workflow Monitor
    - Purpose: Monitor, troubleshoot, and manage running workflows

# Key Features:

Monitoring Capabilities:
    - Real-time Status: Running, succeeded, failed workflows
    - Performance Metrics: Row counts, throughput, timing
    - Error Tracking: Session logs, error messages
    - Resource Usage: Memory, CPU utilization

# Views Available:

    Navigator View
    ├── Repository Navigator (Browse objects)
    ├── Task View (Individual task status)
    ├── Gantt Chart View (Timeline visualization)
    └── Output Window (Detailed logs)

# Operations:
    - Start/Stop workflows
    - View session logs
    - Recover failed sessions
    - Generate reports
    - Set up alerts and notifications

# Running Mappings 

# Process Flow:

    1. Create Mapping (Designer)
    2. Create Session (Workflow Manager)
    3. Create Workflow (Workflow Manager)
    4. Run Workflow (Workflow Monitor)
    5. Monitor Execution (Workflow Monitor)

# Session Execution Steps:
    1. Read: Integration Service reads session properties
    2. Load: Loads mapping metadata
    3. Execute: Processes data according to mapping logic
    4. Write: Loads transformed data to targets
    5. Commit: Finalizes transaction

### Performance Tuning:
    - Partitioning: Parallel processing
    - Caching: Lookup and aggregator optimization
    - Pushdown Optimization: Database-level processing
    - Session Configuration: Memory allocation, commit intervals



# Workflow Creation & Deletion

### Creating Workflows:

#### Step-by-Step Process:

    `powershell
    # 1. Open Workflow Manager
    # 2. Connect to Repository
    # 3. Create New Workflow
    `

    `sql
    -- Workflow Creation Components:
    START → Session_Task → Email_Task → END
    ↓         ↓           ↓
    Always   OnSuccess   OnFailure
    `

# Best Practices:
    - Use meaningful naming conventions
    - Add error handling tasks
    - Include logging and notification
    - Set appropriate timeout values
    - Use worklets for reusability

# Workflow Deletion:

# Safe Deletion Process:
    1. Stop Running Instances: Ensure workflow is not executing
    2. Check Dependencies: Verify no other workflows reference it
    3. Backup: Export workflow definition if needed
    4. Delete: Remove from repository

#### Deletion Considerations:

    Before Deletion Checklist:
    ├── No active sessions running
    ├── No scheduler dependencies  
    ├── No parent workflow references
    ├── Backup/export completed
    └── Stakeholder approval obtained


# Workflow Scheduling:
    - pmcmd: Command-line scheduling
    - Third-party Schedulers: Integration with enterprise schedulers
    - Event-based: Trigger workflows based on file arrival/database changes



# Summary Architecture Flow:

Designer → Workflow Manager → Repository → Integration Service → Workflow Monitor
    ↓           ↓              ↓            ↓                    ↓
 Mappings   Workflows     Metadata     Execution            Monitoring


This architecture provides a robust, scalable platform for enterprise data integration with clear separation of development, management, and monitoring concerns.

### END OF LECTURE 5 UNDER SESSION 1 ###